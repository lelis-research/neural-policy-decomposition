import copy
import itertools
import math
import numpy as np
from typing import Union, List
from agents.policy_guided_agent import PPOAgent
from environemnts.environments_combogrid_gym import ComboGym
from environemnts.environments_minigrid import get_training_tasks_simplecross


class LevinLossActorCritic:
    def is_applicable(self, trajectory, actions, start_index):
        """
        This function checks whether an MLP is applicable in a given state. 

        An actor-critic agent is applicable if the sequence of actions it produces matches
        the sequence of actions in the trajectory. Note that we do not consider an
        actor-critic agent if it has less than 2 actions, as it would be equivalent to a 
        primitive action. 
        """
        if len(actions) <= 1 or len(actions) + start_index > len(trajectory):
            return False
        
        for i in range(len(actions)):
            if actions[i] != trajectory[i + start_index][1]:
                return False
        return True

    def _run(self, env, mask, agent, numbers_steps):
        """
        This function executes an option, which is given by a mask, an agent, and a number of steps. 

        It runs the masked model of the agent for the specified number of steps and it returns the actions taken for those steps. 
        """
        trajectory = agent.run_with_mask(env, mask, numbers_steps)

        actions = []
        for _, action in trajectory.get_trajectory():
            actions.append(action)

        return actions

    def loss(self, masks, models, trajectory, number_actions, joint_problem_name_list, problem_str, number_steps):
        """
        This function implements the dynamic programming method from Alikhasi & Lelis (2024). 

        Note that the if-statement with the following code is in a different place. I believe there is
        a bug in the pseudocode of Alikhasi & Lelis (2024).

        M[j] = min(M[j - 1] + 1, M[j])
        """
        t = trajectory.get_trajectory()
        M = np.arange(len(t) + 1)

        for j in range(len(t) + 1):
            if j > 0:
                M[j] = min(M[j - 1] + 1, M[j])
            if j < len(t):
                for i in range(len(masks)):
                    # the mask being considered for selection cannot be evaluated on the trajectory
                    # generated by the MLP trained to solve the problem.
                    if joint_problem_name_list[j] == problem_str:
                        continue
                    actions = self._run(copy.deepcopy(t[j][0]), masks[i], models[i], number_steps[i])

                    if self.is_applicable(t, actions, j):
                        M[j + len(actions)] = min(M[j + len(actions)], M[j] + 1)
        uniform_probability = (1/(len(masks) + number_actions)) 
        depth = len(t) + 1
        number_decisions = M[len(t)]

        # use the Levin loss in log space to avoid numerical issues
        log_depth = math.log(depth)
        log_uniform_probability = math.log(uniform_probability)
        return log_depth - number_decisions * log_uniform_probability

    def compute_loss(self, masks, models, problem_str, trajectories, number_actions, number_steps):
        """
        This function computes the Levin loss of a set of masks (programs). Each mask in the set is 
        what we select as a set of options, according to Alikhasi & Lelis (2024). 

        The loss is computed for a set of trajectories, one for each training task. Instead of taking
        the average loss across all trajectories, in this function we stich all trajectories together
        forming one long trajectory. The function is implemented this way for debugging purposes. 
        Since a mask k extracted from MLP b cannot be evaluated in the trajectory
        b generated, this "leave one out" was more difficult to debug. Stiching all trajectories
        into a single one makes it easier (see chained_trajectory below). 

        We still do not evaluate a mask on the data it was used to generate it. This is achieved
        with the vector joint_problem_name_list below, which is passed to the loss function. 
        """
        chained_trajectory = None
        joint_problem_name_list = []
        for problem, trajectory in trajectories.items():

            if chained_trajectory is None:
                chained_trajectory = copy.deepcopy(trajectory)
            else:
                chained_trajectory._sequence = chained_trajectory._sequence + copy.deepcopy(trajectory._sequence)
            name_list = [problem for _ in range(len(trajectory._sequence))]
            joint_problem_name_list = joint_problem_name_list + name_list
        return self.loss(masks, models, chained_trajectory, number_actions, joint_problem_name_list, problem_str, number_steps)

    def print_output_subpolicy_trajectory(self, models, masks, masks_problems, trajectories, number_steps, logger):
        """
        This function prints the "behavior" of the options encoded in a set of masks. It will show
        when each option is applicable in different states of the different trajectories. Here is 
        a typical output of this function.

        BL-TR
        Mask:  o0
        001001102102001102001102
        -----000----------------
        --------------000-------
        --------------------000-

        Mask:  o3
        001001102102001102001102
        ------333---------------
        ---------------333------
        ----------------333-----
        ---------------------333
        ----------------------33

        Number of Decisions:  18

        It shows how different masks are used in a given sequence. In the example above, option o0
        is used in the sequence 110, while option o3 is used in some of the occurrences of 102. 
        """
        for problem, trajectory in trajectories.items():  
            logger.info(f"Option Occurrences in {problem}")

            mask_usage = {}
            t = trajectory.get_trajectory()
            M = np.arange(len(t) + 1)

            for j in range(len(t) + 1):
                if j > 0:
                    if M[j - 1] + 1 < M[j]:
                        M[j] = M[j - 1] + 1

                if j < len(t):
                    for i in range(len(masks)):

                        if masks_problems[i] == problem:
                            continue

                        actions = self._run(copy.deepcopy(t[j][0]), masks[i], models[i], number_steps[i])

                        if self.is_applicable(t, actions, j):
                            M[j + len(actions)] = min(M[j + len(actions)], M[j] + 1)

                            mask_name = 'o' + str(i) + "-" + str(masks[i].cpu().numpy())
                            if mask_name not in mask_usage:
                                mask_usage[mask_name] = []

                            usage = ['-' for _ in range(len(t))]
                            for k in range(j, j+len(actions)):
                                usage[k] = str(i)
                            mask_usage[mask_name].append(usage)

            
            for mask, matrix in mask_usage.items():
                logger.info(f'Mask: {mask}')
                buffer = "\n"
                for _, action in t:
                    buffer += str(action)
                buffer += "\n"
                for use in matrix:
                    for v in use:
                        buffer += str(v)
                    buffer += "\n"
                logger.info(buffer)
            logger.info(f'Number of Decisions:  {M[len(t)]}')

    def evaluate_on_each_cell(self, test_agents: List[PPOAgent], masks, trained_problems, problem_test, args: Args, seed: int, label="", logger=None):
        """
        This test is to see for each cell, options will give which sequence of actions
        """
        if args.env_id == "MiniGrid-SimpleCrossingS9N1-v0":
            env = get_training_tasks_simplecross(args.game_width, seed=seed)
            directions = ["R", "D", "L", "U"]
            game_width = 7
        elif args.env_id == "ComboGrid":
            env = ComboGym(args.game_width, args.game_width, problem_test)
            directions = ["NA"]
            game_width = args.game_width
        else:
            raise NotImplementedError
        for agent, idx, mask, trained_problem in zip(test_agents, range(len(test_agents)), masks, trained_problems):
            # Evaluating the performance of options
            logger.info(f"\n {label} {idx} Option: {mask.cpu().numpy()} {trained_problem}")
            for direction in directions:
                logger.info(f"Direction: {direction}")
                options = {}
                for i in range(game_width):
                    for j in range(game_width):    
                        if env.is_over(loc=(i,j)):
                            continue
                        env.reset(init_loc=(i,j), init_dir=direction)
                        trajectory = agent.run_with_mask(env, mask, max_size_sequence=agent.option_size)
                        actions = trajectory.get_action_sequence()
                        options[(i,j)] = actions
                state = trajectory.get_state_sequence()[0]

                logger.info("Option Outputs:")
                buffer = "\n"
                for i in range(game_width):
                    for j in range(game_width):
                        if env.is_over(loc=(i,j)):
                            buffer += ("Goal" + " " * 10)[:10]
                            continue
                        buffer += (",".join(list(map(str, options[(i,j)]))) + " " * 10)[:10]
                    buffer += "\n"
                logger.info(buffer)
                # logger.info("\n" + state.represent_options(options))

                # Evaluating the performance of original agents
                options = {}
                for i in range(game_width):
                    for j in range(game_width):    
                        if env.is_over(loc=(i,j)):
                            continue
                        env.reset(init_loc=(i,j), init_dir=direction)
                        trajectory = agent.run(env, length_cap=agent.option_size - 1)
                        actions = trajectory.get_action_sequence()
                        options[(i,j)] = actions
                state = trajectory.get_state_sequence()[0]

                logger.info("Original Agent's Outputs:")
                buffer = "\n"
                for i in range(game_width):
                    for j in range(game_width):
                        if env.is_over(loc=(i,j)):
                            buffer += ("Goal" + " " * 10)[:10]
                            continue
                        buffer += (",".join(list(map(str, options[(i,j)]))) + " " * 10)[:10]
                    buffer += "\n"
                logger.info(buffer)
                # logger.info("\n" + state.represent_options(options))
        logger.info("#### ### ###\n")


class LogitsLossActorCritic(LevinLossActorCritic): 
    def _run_for_logits(self, env, mask, agent, numbers_steps):
        """
        This function executes an option, which is given by a mask, an agent, and a number of steps. 

        It runs the masked model of the agent for the specified number of steps and it returns the actions taken for those steps. 
        """
        trajectory = agent.run_with_mask(env, mask, numbers_steps)

        actions = []
        logits_ls = []
        for ( _, action), logits in zip(trajectory.get_trajectory(), trajectory.get_logits()):
            actions.append(action)
            logits_ls.append(logits)

        return actions, logits_ls

    def loss(self, masks, models, trajectory, number_actions, joint_problem_name_list, problem_str, number_steps):
        """
        This function implements the dynamic programming method from Alikhasi & Lelis (2024). 

        Note that the if-statement with the following code is in a different place. I believe there is
        a bug in the pseudocode of Alikhasi & Lelis (2024).

        M[j] = min(M[j - 1] + 1, M[j])
        """
        t = trajectory.get_trajectory()
        M = np.arange(len(t) + 1)

        for j in range(len(t) + 1):
            if j > 0:
                M[j] = min(M[j - 1] + 1, M[j])
            if j < len(t):
                for i in range(len(masks)):
                    # the mask being considered for selection cannot be evaluated on the trajectory
                    # generated by the MLP trained to solve the problem.
                    if joint_problem_name_list[j] == problem_str:
                        continue
                    actions, logits = self._run_for_logits(copy.deepcopy(t[j][0]), masks[i], models[i], number_steps[i])

                    if self.is_applicable(t, actions, j):
                        M[j + len(actions)] = min(M[j + len(actions)], M[j] + 1)
        uniform_probability = (1/(len(masks) + number_actions)) 
        depth = len(t) + 1
        number_decisions = M[len(t)]

        # use the Levin loss in log space to avoid numerical issues
        log_depth = math.log(depth)
        log_uniform_probability = math.log(uniform_probability)
        return log_depth - number_decisions * log_uniform_probability

    def compute_loss(self, masks, models, problem_str, trajectories, number_actions, number_steps):
        """
        This function computes the Levin loss of a set of masks (programs). Each mask in the set is 
        what we select as a set of options, according to Alikhasi & Lelis (2024). 

        The loss is computed for a set of trajectories, one for each training task. Instead of taking
        the average loss across all trajectories, in this function we stich all trajectories together
        forming one long trajectory. The function is implemented this way for debugging purposes. 
        Since a mask k extracted from MLP b cannot be evaluated in the trajectory
        b generated, this "leave one out" was more difficult to debug. Stiching all trajectories
        into a single one makes it easier (see chained_trajectory below). 

        We still do not evaluate a mask on the data it was used to generate it. This is achieved
        with the vector joint_problem_name_list below, which is passed to the loss function. 
        """
        chained_trajectory = None
        joint_problem_name_list = []
        for problem, trajectory in trajectories.items():

            if chained_trajectory is None:
                chained_trajectory = copy.deepcopy(trajectory)
            else:
                chained_trajectory._sequence = chained_trajectory._sequence + copy.deepcopy(trajectory._sequence)
            name_list = [problem for _ in range(len(trajectory._sequence))]
            joint_problem_name_list = joint_problem_name_list + name_list
        return self.loss(masks, models, chained_trajectory, number_actions, joint_problem_name_list, problem_str, number_steps)

    def print_output_subpolicy_trajectory(self, models, masks, masks_problems, trajectories, number_steps, logger):
        """
        This function prints the "behavior" of the options encoded in a set of masks. It will show
        when each option is applicable in different states of the different trajectories. Here is 
        a typical output of this function.

        BL-TR
        Mask:  o0
        001001102102001102001102
        -----000----------------
        --------------000-------
        --------------------000-

        Mask:  o3
        001001102102001102001102
        ------333---------------
        ---------------333------
        ----------------333-----
        ---------------------333
        ----------------------33

        Number of Decisions:  18

        It shows how different masks are used in a given sequence. In the example above, option o0
        is used in the sequence 110, while option o3 is used in some of the occurrences of 102. 
        """
        for problem, trajectory in trajectories.items():  
            logger.info(f"Option Occurrences in {problem}")

            mask_usage = {}
            t = trajectory.get_trajectory()
            M = np.arange(len(t) + 1)

            for j in range(len(t) + 1):
                if j > 0:
                    if M[j - 1] + 1 < M[j]:
                        M[j] = M[j - 1] + 1

                if j < len(t):
                    for i in range(len(masks)):

                        if masks_problems[i] == problem:
                            continue

                        actions = self._run(copy.deepcopy(t[j][0]), masks[i], models[i], number_steps[i])

                        if self.is_applicable(t, actions, j):
                            M[j + len(actions)] = min(M[j + len(actions)], M[j] + 1)

                            mask_name = 'o' + str(i) + "-" + str(masks[i].cpu().numpy())
                            if mask_name not in mask_usage:
                                mask_usage[mask_name] = []

                            usage = ['-' for _ in range(len(t))]
                            for k in range(j, j+len(actions)):
                                usage[k] = str(i)
                            mask_usage[mask_name].append(usage)

            
            for mask, matrix in mask_usage.items():
                logger.info(f'Mask: {mask}')
                buffer = "\n"
                for _, action in t:
                    buffer += str(action)
                buffer += "\n"
                for use in matrix:
                    for v in use:
                        buffer += str(v)
                    buffer += "\n"
                logger.info(buffer)
            logger.info(f'Number of Decisions:  {M[len(t)]}')

    def evaluate_on_each_cell(self, test_agents: List[PPOAgent], masks, trained_problems, problem_test, args: Args, seed: int, label="", logger=None):
        """
        This test is to see for each cell, options will give which sequence of actions
        """
        if args.env_id == "MiniGrid-SimpleCrossingS9N1-v0":
            env = get_training_tasks_simplecross(args.game_width, seed=seed)
            directions = ["R", "D", "L", "U"]
            game_width = 7
        elif args.env_id == "ComboGrid":
            env = ComboGym(args.game_width, args.game_width, problem_test)
            directions = ["NA"]
            game_width = args.game_width
        else:
            raise NotImplementedError
        for agent, idx, mask, trained_problem in zip(test_agents, range(len(test_agents)), masks, trained_problems):
            # Evaluating the performance of options
            logger.info(f"\n {label} {idx} Option: {mask.cpu().numpy()} {trained_problem}")
            for direction in directions:
                logger.info(f"Direction: {direction}")
                options = {}
                for i in range(game_width):
                    for j in range(game_width):    
                        if env.is_over(loc=(i,j)):
                            continue
                        env.reset(init_loc=(i,j), init_dir=direction)
                        trajectory = agent.run_with_mask(env, mask, max_size_sequence=agent.option_size)
                        actions = trajectory.get_action_sequence()
                        options[(i,j)] = actions
                state = trajectory.get_state_sequence()[0]

                logger.info("Option Outputs:")
                buffer = "\n"
                for i in range(game_width):
                    for j in range(game_width):
                        if env.is_over(loc=(i,j)):
                            buffer += ("Goal" + " " * 10)[:10]
                            continue
                        buffer += (",".join(list(map(str, options[(i,j)]))) + " " * 10)[:10]
                    buffer += "\n"
                logger.info(buffer)
                # logger.info("\n" + state.represent_options(options))

                # Evaluating the performance of original agents
                options = {}
                for i in range(game_width):
                    for j in range(game_width):    
                        if env.is_over(loc=(i,j)):
                            continue
                        env.reset(init_loc=(i,j), init_dir=direction)
                        trajectory = agent.run(env, length_cap=agent.option_size - 1)
                        actions = trajectory.get_action_sequence()
                        options[(i,j)] = actions
                state = trajectory.get_state_sequence()[0]

                logger.info("Original Agent's Outputs:")
                buffer = "\n"
                for i in range(game_width):
                    for j in range(game_width):
                        if env.is_over(loc=(i,j)):
                            buffer += ("Goal" + " " * 10)[:10]
                            continue
                        buffer += (",".join(list(map(str, options[(i,j)]))) + " " * 10)[:10]
                    buffer += "\n"
                logger.info(buffer)
                # logger.info("\n" + state.represent_options(options))
        logger.info("#### ### ###\n")